{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf,collect_list,array_contains\n",
    "\n",
    "from pyspark.ml.feature import IDF, CountVectorizer,StringIndexer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.linalg import SparseVector, VectorUDT,DenseVector\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from metric_learn import SDML\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_df = spark.read.csv(\"../../mimic3/data/DIAGNOSES_ICD.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "dic_ICD_df = spark.read.csv(\"../../mimic3/data/D_ICD_DIAGNOSES.csv\", header=True, mode=\"DROPMALFORMED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_df.createOrReplaceTempView(\"diagnosis\")\n",
    "\n",
    "#Fitering the unspecified disgnosis codes\n",
    "filteredDiags = spark.sql(\"SELECT SUBJECT_ID,ICD9_CODE FROM diagnosis WHERE ICD9_CODE not in ('', '4019','7793','2724','2449')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract frequent diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing top 500 frequent diagnosis codes.\n",
    "topDiags = filteredDiags.groupBy(\"ICD9_CODE\").count().sort(col(\"count\").desc()).select(\"ICD9_CODE\").limit(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract patients who had the requent diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner join to get patients who had the top 500 diagnosis codes.\n",
    "top_freq_pats = filteredDiags.join(topDiags, filteredDiags.ICD9_CODE == topDiags.ICD9_CODE, \"inner\").\\\n",
    "                drop(topDiags.ICD9_CODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Aggregate diagnosis codes list through grouping by \"SUBJECT_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats_dias = top_freq_pats.groupBy(\"SUBJECT_ID\").agg(collect_list(\"ICD9_CODE\"))\n",
    "pats_dias = pats_dias.select(col(\"SUBJECT_ID\"),col(\"collect_list(ICD9_CODE)\").alias(\"codes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Encode patients and diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index subject_id to label\n",
    "indexer = StringIndexer(inputCol=\"SUBJECT_ID\", outputCol=\"label\")\n",
    "indexed = indexer.fit(pats_dias).transform(pats_dias)\n",
    "\n",
    "#terms' count vector\n",
    "vector = CountVectorizer(inputCol=\"codes\", outputCol=\"tf_features\")\n",
    "countVect = vector.fit(indexed)  \n",
    "\n",
    "#get vocaulary\n",
    "vocabs = countVect.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Frequency of diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqVect = countVect.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SUBJECT_ID: string (nullable = true)\n",
      " |-- codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- tf_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freqVect.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 TF-IDF of diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = freqVect.select(col(\"label\"),col(\"features\").alias(\"rawFeatures\"))\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "idfModel = idf.fit(freqVect)\n",
    "tf_idfVect = idfModel.transform(freqVect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SUBJECT_ID: string (nullable = true)\n",
      " |-- codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- tf_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf_idfVect = tf_idfVect.selectExpr(\"features as TFIDF_features\")\n",
    "tf_idfVect.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 OneHot of diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse(c):\n",
    "    def to_sparse_(v):\n",
    "#         if isinstance(v, SparseVector):\n",
    "#             return v\n",
    "        vs = (v.toArray()>0)*1\n",
    "        nonzero = np.nonzero(vs)[0]\n",
    "        return SparseVector(v.size, nonzero, vs[nonzero])\n",
    "    return udf(to_sparse_, VectorUDT())(c)\n",
    "\n",
    "oneHot_freqVect = tf_idfVect.withColumn(\"oneHot_features\",to_sparse(tf_idfVect.tf_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 AutoEncoders of diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32 \n",
    "\n",
    "# this is our input placeholder\n",
    "input_patient = Input(shape=(500,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_patient)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(500, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_patient, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Spark ML Vector to Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to Pandaâ€™s dataframe\n",
    "array_features = oneHot_freqVect.select('oneHot_features').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Sparse Vector to Matrix\n",
    "dataset = array_features['oneHot_features'].apply(lambda x : x.toArray()).as_matrix().reshape(-1,1)\n",
    "\n",
    "#Flatten using apply_along_axis\n",
    "features = np.apply_along_axis(lambda x : x[0], 1, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(features, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36904 samples, validate on 9227 samples\n",
      "Epoch 1/50\n",
      "36904/36904 [==============================] - 1s 30us/step - loss: 0.6220 - val_loss: 0.4284\n",
      "Epoch 2/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.2050 - val_loss: 0.1150\n",
      "Epoch 3/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.1042 - val_loss: 0.0969\n",
      "Epoch 4/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0950 - val_loss: 0.0921\n",
      "Epoch 5/50\n",
      "36904/36904 [==============================] - 1s 28us/step - loss: 0.0914 - val_loss: 0.0893\n",
      "Epoch 6/50\n",
      "36904/36904 [==============================] - 1s 28us/step - loss: 0.0889 - val_loss: 0.0872\n",
      "Epoch 7/50\n",
      "36904/36904 [==============================] - 1s 29us/step - loss: 0.0868 - val_loss: 0.0854\n",
      "Epoch 8/50\n",
      "36904/36904 [==============================] - 1s 28us/step - loss: 0.0851 - val_loss: 0.0837\n",
      "Epoch 9/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0835 - val_loss: 0.0823\n",
      "Epoch 10/50\n",
      "36904/36904 [==============================] - 1s 28us/step - loss: 0.0820 - val_loss: 0.0810\n",
      "Epoch 11/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0808 - val_loss: 0.0799\n",
      "Epoch 12/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0797 - val_loss: 0.0789\n",
      "Epoch 13/50\n",
      "36904/36904 [==============================] - 1s 26us/step - loss: 0.0787 - val_loss: 0.0780\n",
      "Epoch 14/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0778 - val_loss: 0.0772\n",
      "Epoch 15/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0771 - val_loss: 0.0765\n",
      "Epoch 16/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0764 - val_loss: 0.0758\n",
      "Epoch 17/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0757 - val_loss: 0.0752\n",
      "Epoch 18/50\n",
      "36904/36904 [==============================] - 1s 28us/step - loss: 0.0752 - val_loss: 0.0746\n",
      "Epoch 19/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0746 - val_loss: 0.0741\n",
      "Epoch 20/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0741 - val_loss: 0.0737\n",
      "Epoch 21/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0737 - val_loss: 0.0732\n",
      "Epoch 22/50\n",
      "36904/36904 [==============================] - 1s 28us/step - loss: 0.0732 - val_loss: 0.0728\n",
      "Epoch 23/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0728 - val_loss: 0.0724\n",
      "Epoch 24/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0725 - val_loss: 0.0721\n",
      "Epoch 25/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0721 - val_loss: 0.0717\n",
      "Epoch 26/50\n",
      "36904/36904 [==============================] - 1s 26us/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 27/50\n",
      "36904/36904 [==============================] - 1s 26us/step - loss: 0.0714 - val_loss: 0.0711\n",
      "Epoch 28/50\n",
      "36904/36904 [==============================] - 1s 26us/step - loss: 0.0711 - val_loss: 0.0708\n",
      "Epoch 29/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0708 - val_loss: 0.0705\n",
      "Epoch 30/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0705 - val_loss: 0.0702\n",
      "Epoch 31/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0703 - val_loss: 0.0699\n",
      "Epoch 32/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0700 - val_loss: 0.0696\n",
      "Epoch 33/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0697 - val_loss: 0.0694\n",
      "Epoch 34/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0694 - val_loss: 0.0691\n",
      "Epoch 35/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0692 - val_loss: 0.0688\n",
      "Epoch 36/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0689 - val_loss: 0.0686\n",
      "Epoch 37/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0687 - val_loss: 0.0684\n",
      "Epoch 38/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0684 - val_loss: 0.0681\n",
      "Epoch 39/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0682 - val_loss: 0.0679\n",
      "Epoch 40/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0680 - val_loss: 0.0676\n",
      "Epoch 41/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0677 - val_loss: 0.0674\n",
      "Epoch 42/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0675 - val_loss: 0.0672\n",
      "Epoch 43/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0673 - val_loss: 0.0670\n",
      "Epoch 44/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0671 - val_loss: 0.0668\n",
      "Epoch 45/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0668 - val_loss: 0.0666\n",
      "Epoch 46/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0666 - val_loss: 0.0663\n",
      "Epoch 47/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0664 - val_loss: 0.0661\n",
      "Epoch 48/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0662 - val_loss: 0.0659\n",
      "Epoch 49/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0660 - val_loss: 0.0657\n",
      "Epoch 50/50\n",
      "36904/36904 [==============================] - 1s 27us/step - loss: 0.0658 - val_loss: 0.0656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af77981cb38>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_patient, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_patients = encoder.predict(X_test)\n",
    "# decoded_patients = decoder.predict(encoded_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SUBJECT_ID: string (nullable = true)\n",
      " |-- codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- tf_features: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      " |-- oneHot_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oneHot_freqVect.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "|SUBJECT_ID|               codes|  label|         tf_features|      tfidf_features|     oneHot_features|\n",
      "+----------+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "|     10096|[4260, 41401, 530...| 9643.0|(500,[2,7,39,51,2...|(500,[2,7,39,51,2...|(500,[2,7,39,51,2...|\n",
      "|     10351|[7661, V290, V293...|19592.0|(500,[9,10,32,267...|(500,[9,10,32,267...|(500,[9,10,32,267...|\n",
      "|     10436|         [431, 3314]|28247.0|(500,[78,212],[1....|(500,[78,212],[3....|(500,[78,212],[1....|\n",
      "|      1090|[V3001, V502, V05...|43282.0|(500,[9,10,32,52]...|(500,[9,10,32,52]...|(500,[9,10,32,52]...|\n",
      "|     11078|[41071, 41401, 42...|44246.0|(500,[2,4,28,30,1...|(500,[2,4,28,30,1...|(500,[2,4,28,30,1...|\n",
      "|     11332|[99811, 48283, 42...| 7917.0|(500,[0,1,63,287,...|(500,[0,1,63,287,...|(500,[0,1,63,287,...|\n",
      "|     11563|[51881, 03849, 57...|  930.0|(500,[4,5,15,16,3...|(500,[4,5,15,16,3...|(500,[4,5,15,16,3...|\n",
      "|      1159|        [2948, 3659]|33288.0|(500,[105,168],[1...|(500,[105,168],[3...|(500,[105,168],[1...|\n",
      "|     11722|[41071, 53140, 42...|12808.0|(500,[0,1,2,3,4,5...|(500,[0,1,2,3,4,5...|(500,[0,1,2,3,4,5...|\n",
      "|     11888|[V3000, V290, V50...| 4710.0|(500,[9,10,20,52,...|(500,[9,10,20,52,...|(500,[9,10,20,52,...|\n",
      "|     12394|[V3000, 77089, V2...|26450.0|(500,[10,20,208,2...|(500,[10,20,208,2...|(500,[10,20,208,2...|\n",
      "|     12529|[V3000, V290, V05...|22296.0|(500,[9,10,20,327...|(500,[9,10,20,327...|(500,[9,10,20,327...|\n",
      "|     12847|[03811, 4210, 785...| 4656.0|(500,[1,16,33,36,...|(500,[1,16,33,36,...|(500,[1,16,33,36,...|\n",
      "|     13192|[41071, 5849, 403...|36761.0|(500,[0,2,3,4,5,6...|(500,[0,2,3,4,5,6...|(500,[0,2,3,4,5,6...|\n",
      "|     13282|[41519, 340, 2765...|24348.0|(500,[24,77,141,1...|(500,[24,77,141,1...|(500,[24,77,141,1...|\n",
      "|     13442|             [07070]|23982.0|   (500,[182],[1.0])|(500,[182],[4.453...|   (500,[182],[1.0])|\n",
      "|     13610|[5070, 7907, 5990...|37341.0|(500,[6,11,19,68,...|(500,[6,11,19,68,...|(500,[6,11,19,68,...|\n",
      "|     13772|[4280, 41402, 414...|19468.0|(500,[0,2,51,59,2...|(500,[0,2,51,59,2...|(500,[0,2,51,59,2...|\n",
      "|     13865|             [V3000]| 1975.0|    (500,[20],[1.0])|(500,[20],[2.5597...|    (500,[20],[1.0])|\n",
      "|     14157| [042, 51881, 25000]| 4922.0|(500,[4,5,225],[1...|(500,[4,5,225],[1...|(500,[4,5,225],[1...|\n",
      "+----------+--------------------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oneHot_freqVect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Topic Model of diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = oneHot_freqVect.select(col(\"label\"),col(\"tf_features\").alias(\"features\"))\n",
    "\n",
    "#train LDA model\n",
    "lda_mimic = LDA(k=10, maxIter=10)\n",
    "model_mimic = lda_mimic.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe topics.\n",
    "tf_topics = model_mimic.describeTopics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distribution matrix of documents to topics\n",
    "docTopics = model_mimic.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Features from AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering,KMeans,MiniBatchKMeans,AgglomerativeClustering,Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all patients' autoencoders\n",
    "encoded_patients = encoder.predict(features)\n",
    "ae_patients = pd.DataFrame(encoded_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a new column as subject_id\n",
    "ae_patients['SUBJECT_ID'] = oneHot_freqVect.select('SUBJECT_ID').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 222, 225)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if there are some patients have the two diagnosis codes.\n",
    "patient2  = oneHot_freqVect.where(array_contains(oneHot_freqVect.codes,\"725\"))\n",
    "patient1  = oneHot_freqVect.where(array_contains(oneHot_freqVect.codes,\"77181\"))\n",
    "patients = patient1.join(patient2, patient1.SUBJECT_ID == patient2.SUBJECT_ID, 'inner')\n",
    "patients.count(),patient1.count(),patient2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert spark dataframe to pandas dataframe\n",
    "patient1_pd  = patient1.select('SUBJECT_ID').toPandas()\n",
    "patient2_pd  = patient2.select('SUBJECT_ID').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46131, 33)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_patients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get patients' autoencoder codes\n",
    "profs1 = pd.merge(ae_patients,patient1_pd,how='inner',on='SUBJECT_ID').drop(['SUBJECT_ID'], axis=1)\n",
    "profs2 = pd.merge(ae_patients,patient2_pd,how='inner',on='SUBJECT_ID').drop(['SUBJECT_ID'], axis=1)\n",
    "profs = pd.concat([profs1,profs2],axis=0)\n",
    "\n",
    "#Assign label to each patient\n",
    "pat1 = pd.DataFrame({'label':np.ones(patient1_pd.shape[0])})\n",
    "pat2 = pd.DataFrame({'label':np.zeros(patient2_pd.shape[0])})\n",
    "labels = pd.concat([pat1,pat2],axis=0)\n",
    "labels.label = labels.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans(n_clusters=2).fit_predict(profs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['pred'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0035309639150432623,\n",
       " 0.003047740592616972,\n",
       " (0.0084510646969259, 0.005151601020241131, 0.006401173772852876),\n",
       " 0.610738698304978)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_mutual_info_score(labels.pred, labels.label),\\\n",
    "metrics.adjusted_rand_score(labels.pred, labels.label),\\\n",
    "metrics.homogeneity_completeness_v_measure(labels.pred, labels.label),\\\n",
    "metrics.fowlkes_mallows_score(labels.pred, labels.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = labels[labels['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSample(labels):\n",
    "    sim_samples_list = []\n",
    "    uniq_lables = labels.label.unique()\n",
    "    for label in range(uniq_lables.size):\n",
    "        samples = labels[labels['label']==uniq_lables[label]]\n",
    "        uniq_cluster = samples.pred.unique()\n",
    "        if uniq_cluster.size == 1: continue\n",
    "        for first_cluster_index in range(uniq_cluster.size-1):\n",
    "            first_cluster_samples = samples[samples['pred']==uniq_cluster[first_cluster_index]]\n",
    "            first_cluster_samples.is_copy = False\n",
    "            first_cluster_samples['id'] = first_cluster_samples.index.tolist()\n",
    "            for second_cluster_index in range(first_cluster_index+1,uniq_cluster.size):\n",
    "                second_cluster_samples = samples[samples['pred']==uniq_cluster[second_cluster_index]]\n",
    "                second_cluster_samples.is_copy = False\n",
    "                second_cluster_samples['id'] = second_cluster_samples.index.tolist()\n",
    "                first_cluster_samples['key'] = 1\n",
    "                second_cluster_samples['key'] = 1\n",
    "                df = pd.merge(first_cluster_samples, second_cluster_samples, on='key')\n",
    "                del df['key']\n",
    "                sim_samples_list.append(df)\n",
    "    if(len(sim_samples_list)> 0):\n",
    "        sim_samples =  pd.concat(sim_samples_list, axis=0)    \n",
    "        sim_samples = sim_samples.reset_index()\n",
    "        index = random.randint(0,sim_samples.shape[0])\n",
    "        return sim_samples.iloc[[index]]\n",
    "    else:\n",
    "        return 'null';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sample = getSample(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label_x</th>\n",
       "      <th>pred_x</th>\n",
       "      <th>id_x</th>\n",
       "      <th>label_y</th>\n",
       "      <th>pred_y</th>\n",
       "      <th>id_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5849.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5849.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5849.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5849.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5849.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5849.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  label_x  pred_x   id_x  label_y  pred_y  id_y\n",
       "count     1.0      1.0     1.0    1.0      1.0     1.0   1.0\n",
       "mean   5849.0      1.0     0.0  171.0      1.0     1.0  31.0\n",
       "std       NaN      NaN     NaN    NaN      NaN     NaN   NaN\n",
       "min    5849.0      1.0     0.0  171.0      1.0     1.0  31.0\n",
       "25%    5849.0      1.0     0.0  171.0      1.0     1.0  31.0\n",
       "50%    5849.0      1.0     0.0  171.0      1.0     1.0  31.0\n",
       "75%    5849.0      1.0     0.0  171.0      1.0     1.0  31.0\n",
       "max    5849.0      1.0     0.0  171.0      1.0     1.0  31.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricLearning(samples, df_data):\n",
    "    \n",
    "     #get unique row ids\n",
    "    rowIDLIst = pd.concat([samples.id_x,samples.id_y],axis = 0).unique().tolist()\n",
    "\n",
    "    #connectivity graph\n",
    "    cmatrix = np.zeros([len(rowIDLIst),len(rowIDLIst)])\n",
    "    \n",
    "    for index,row in samples.iterrows():\n",
    "        cmatrix[rowIDLIst.index(row.id_x)][rowIDLIst.index(row.id_y)] = 1\n",
    "        cmatrix[rowIDLIst.index(row.id_y)][rowIDLIst.index(row.id_x)] = 1\n",
    "        \n",
    "#     print(cmatrix)\n",
    "    trainedData = []\n",
    "    for rid in rowIDLIst:\n",
    "        row = df_data.iloc[[rid]]\n",
    "        trainedData.append(row) \n",
    "    trainedData = pd.concat(trainedData,axis = 0).as_matrix()   \n",
    "    \n",
    "#     print(trainedData)\n",
    "    metric = SDML(use_cov=False).fit(trainedData, cmatrix)  \n",
    "    newData = metric.transform(df_data) \n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "0.0035309639150432623\n",
      "66\n",
      "0.0028395333133259437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "0.0028395333133259437\n",
      "67\n",
      "0.0035309639150432623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "0.0035309639150432623\n",
      "381\n",
      "0.002839533313325943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "0.0028395333133259437\n",
      "380\n",
      "0.0035309639150432623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "0.0035309639150432623\n",
      "67\n",
      "0.0035309639150432623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/scratch/xp_python3.6/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function pinvh is deprecated; sklearn.utils.extmath.pinvh was deprecated in version 0.19 and will be removed in 0.21. Use scipy.linalg.pinvh instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "df_data = profs\n",
    "for i in range(10):\n",
    "    clusters = KMeans(n_clusters=2).fit_predict(df_data)\n",
    "    print(np.count_nonzero(clusters))\n",
    "    labels['pred'] = clusters\n",
    "    print(metrics.adjusted_mutual_info_score(labels.pred, labels.label))\n",
    "    sim_sample = getSample(labels)\n",
    "    if(isinstance(sim_sample, pd.DataFrame)):\n",
    "        df_data = metricLearning(sim_sample,df_data)\n",
    "        newData = preprocessing.StandardScaler().fit_transform(df_data)\n",
    "        df_data = pd.DataFrame(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3374678063574593"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_mutual_info_score([1,1,1,0,0,0], [0,0,0,1,1,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
